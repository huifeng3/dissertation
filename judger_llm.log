INFO 01-24 20:21:07 [__init__.py:239] Automatically detected platform cuda.
INFO 01-24 20:21:08 [api_server.py:981] vLLM API server version 0.8.2
INFO 01-24 20:21:08 [api_server.py:982] args: Namespace(host='0.0.0.0', port=8002, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='./models/HarmBench-Llama-2-13b-cls', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.8, num_gpu_blocks_override=None, max_num_batched_tokens=51200, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)
INFO 01-24 20:21:18 [config.py:585] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 01-24 20:21:18 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=51200.
INFO 01-24 20:21:20 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='./models/HarmBench-Llama-2-13b-cls', speculative_config=None, tokenizer='./models/HarmBench-Llama-2-13b-cls', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=./models/HarmBench-Llama-2-13b-cls, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 01-24 20:21:20 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8796f1c710>
INFO 01-24 20:21:21 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 01-24 20:21:21 [cuda.py:220] Using Flash Attention backend on V1 engine.
INFO 01-24 20:21:21 [gpu_model_runner.py:1174] Starting to load model ./models/HarmBench-Llama-2-13b-cls...
WARNING 01-24 20:21:21 [config.py:3692] `torch.compile` is turned on, but the model ./models/HarmBench-Llama-2-13b-cls does not support it. Please open an issue on GitHub if you want it to be supported.
ERROR 01-24 20:21:21 [core.py:343] EngineCore hit an exception: Traceback (most recent call last):
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 335, in run_engine_core
ERROR 01-24 20:21:21 [core.py:343]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 01-24 20:21:21 [core.py:343]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 290, in __init__
ERROR 01-24 20:21:21 [core.py:343]     super().__init__(vllm_config, executor_class, log_stats)
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 60, in __init__
ERROR 01-24 20:21:21 [core.py:343]     self.model_executor = executor_class(vllm_config)
ERROR 01-24 20:21:21 [core.py:343]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 01-24 20:21:21 [core.py:343]     self._init_executor()
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
ERROR 01-24 20:21:21 [core.py:343]     self.collective_rpc("load_model")
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 01-24 20:21:21 [core.py:343]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 01-24 20:21:21 [core.py:343]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/utils.py", line 2255, in run_method
ERROR 01-24 20:21:21 [core.py:343]     return func(*args, **kwargs)
ERROR 01-24 20:21:21 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 136, in load_model
ERROR 01-24 20:21:21 [core.py:343]     self.model_runner.load_model()
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1177, in load_model
ERROR 01-24 20:21:21 [core.py:343]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 01-24 20:21:21 [core.py:343]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 01-24 20:21:21 [core.py:343]     return loader.load_model(vllm_config=vllm_config)
ERROR 01-24 20:21:21 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 441, in load_model
ERROR 01-24 20:21:21 [core.py:343]     model = _initialize_model(vllm_config=vllm_config)
ERROR 01-24 20:21:21 [core.py:343]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 127, in _initialize_model
ERROR 01-24 20:21:21 [core.py:343]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 01-24 20:21:21 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 480, in __init__
ERROR 01-24 20:21:21 [core.py:343]     self.model = self._init_model(vllm_config=vllm_config,
ERROR 01-24 20:21:21 [core.py:343]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 517, in _init_model
ERROR 01-24 20:21:21 [core.py:343]     return LlamaModel(vllm_config=vllm_config, prefix=prefix)
ERROR 01-24 20:21:21 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 01-24 20:21:21 [core.py:343]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 319, in __init__
ERROR 01-24 20:21:21 [core.py:343]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 01-24 20:21:21 [core.py:343]                                                     ^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 568, in make_layers
ERROR 01-24 20:21:21 [core.py:343]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 01-24 20:21:21 [core.py:343]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 321, in <lambda>
ERROR 01-24 20:21:21 [core.py:343]     lambda prefix: layer_type(config=config,
ERROR 01-24 20:21:21 [core.py:343]                    ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 237, in __init__
ERROR 01-24 20:21:21 [core.py:343]     self.self_attn = LlamaAttention(
ERROR 01-24 20:21:21 [core.py:343]                      ^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/models/llama.py", line 140, in __init__
ERROR 01-24 20:21:21 [core.py:343]     self.qkv_proj = QKVParallelLinear(
ERROR 01-24 20:21:21 [core.py:343]                     ^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 833, in __init__
ERROR 01-24 20:21:21 [core.py:343]     super().__init__(input_size=input_size,
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 398, in __init__
ERROR 01-24 20:21:21 [core.py:343]     self.quant_method.create_weights(
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 178, in create_weights
ERROR 01-24 20:21:21 [core.py:343]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 01-24 20:21:21 [core.py:343]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343]   File "/data1/hhc/miniconda3/envs/TROJail/lib/python3.12/site-packages/torch/utils/_device.py", line 104, in __torch_function__
ERROR 01-24 20:21:21 [core.py:343]     return func(*args, **kwargs)
ERROR 01-24 20:21:21 [core.py:343]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 01-24 20:21:21 [core.py:343] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 94.88 MiB is free. Including non-PyTorch memory, this process has 23.58 GiB memory in use. Of the allocated memory 23.28 GiB is allocated by PyTorch, and 3.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 01-24 20:21:21 [core.py:343] 
CRITICAL 01-24 20:21:21 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.
