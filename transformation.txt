CUDA_VISIBLE_DEVICES: 2,3
(TaskRunner pid=2698159) reward_manager_name: naive
(TaskRunner pid=2698159) using dummy reward manager
(TaskRunner pid=2698159) [validate_config] All configuration checks passed successfully!
(TaskRunner pid=2698159) Total training steps: 260
(TaskRunner pid=2698159) colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
(WorkerDict pid=2699054) Model config after override: Qwen2Config {
(WorkerDict pid=2699054)   "architectures": [
(WorkerDict pid=2699054)     "Qwen2ForCausalLM"
(WorkerDict pid=2699054)   ],
(WorkerDict pid=2699054)   "attention_dropout": 0.0,
(WorkerDict pid=2699054)   "eos_token_id": 151645,
(WorkerDict pid=2699054)   "hidden_act": "silu",
(WorkerDict pid=2699054)   "hidden_size": 2048,
(WorkerDict pid=2699054)   "initializer_range": 0.02,
(WorkerDict pid=2699054)   "intermediate_size": 11008,
(WorkerDict pid=2699054)   "layer_types": [
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention"
(WorkerDict pid=2699054)   ],
(WorkerDict pid=2699054)   "max_position_embeddings": 32768,
(WorkerDict pid=2699054)   "max_window_layers": 70,
(WorkerDict pid=2699054)   "model_type": "qwen2",
(WorkerDict pid=2699054)   "num_attention_heads": 16,
(WorkerDict pid=2699054)   "num_hidden_layers": 36,
(WorkerDict pid=2699054)   "num_key_value_heads": 2,
(WorkerDict pid=2699054)   "pad_token_id": 151643,
(WorkerDict pid=2699054)   "rms_norm_eps": 1e-06,
(WorkerDict pid=2699054)   "rope_scaling": null,
(WorkerDict pid=2699054)   "rope_theta": 1000000.0,
(WorkerDict pid=2699054)   "sliding_window": null,
(WorkerDict pid=2699054)   "tie_word_embeddings": true,
(WorkerDict pid=2699054)   "torch_dtype": "bfloat16",
(WorkerDict pid=2699054)   "transformers_version": "4.53.2",
(WorkerDict pid=2699054)   "use_cache": true,
(WorkerDict pid=2699054)   "use_sliding_window": false,
(WorkerDict pid=2699054)   "vocab_size": 151936
(WorkerDict pid=2699054) }
(WorkerDict pid=2699054) 
(WorkerDict pid=2699490) Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
(WorkerDict pid=2699054) NCCL version 2.21.5+cuda12.4
(WorkerDict pid=2699054) Qwen2ForCausalLM contains 3.09B parameters
(WorkerDict pid=2699054) wrap_policy: functools.partial(<function _or_policy at 0x7fe4c1299bc0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fe4c1299a80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
(WorkerDict pid=2699054) Actor use_remove_padding=True
(WorkerDict pid=2699054) Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
(WorkerDict pid=2699054) Model config after override: Qwen2Config {
(WorkerDict pid=2699054)   "architectures": [
(WorkerDict pid=2699054)     "Qwen2ForCausalLM"
(WorkerDict pid=2699054)   ],
(WorkerDict pid=2699054)   "attention_dropout": 0.0,
(WorkerDict pid=2699054)   "eos_token_id": 151645,
(WorkerDict pid=2699054)   "hidden_act": "silu",
(WorkerDict pid=2699054)   "hidden_size": 2048,
(WorkerDict pid=2699054)   "initializer_range": 0.02,
(WorkerDict pid=2699054)   "intermediate_size": 11008,
(WorkerDict pid=2699054)   "layer_types": [
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention",
(WorkerDict pid=2699054)     "full_attention"
(WorkerDict pid=2699054)   ],
(WorkerDict pid=2699054)   "max_position_embeddings": 32768,
(WorkerDict pid=2699054)   "max_window_layers": 70,
(WorkerDict pid=2699054)   "model_type": "qwen2",
(WorkerDict pid=2699054)   "num_attention_heads": 16,
(WorkerDict pid=2699054)   "num_hidden_layers": 36,
(WorkerDict pid=2699054)   "num_key_value_heads": 2,
(WorkerDict pid=2699054)   "pad_token_id": 151643,
(WorkerDict pid=2699054)   "rms_norm_eps": 1e-06,
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]2699490) 
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.87s/it]
(WorkerDict pid=2699054) Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]
(WorkerDict pid=2699054) /data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
(WorkerDict pid=2699054)   warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.19s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.96s/it]
