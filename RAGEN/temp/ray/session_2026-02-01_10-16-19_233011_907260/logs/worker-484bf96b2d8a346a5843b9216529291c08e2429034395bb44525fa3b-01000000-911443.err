:job_id:01000000
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/gym_sokoban/__init__.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
:actor_name:TaskRunner
DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
WARNING:2026-02-01 10:16:35,855:Waiting for register center actor CQHQOr_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=912421, ip=172.28.6.5, actor_id=81a25c0512d501c44476a5a001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fbfd8596f30>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/verl/verl/single_controller/ray/base.py", line 459, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/verl/verl/single_controller/base/decorator.py", line 465, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/ragen/workers/fsdp_workers.py", line 491, in init_model
    self.actor_module_fsdp, self.actor_optimizer, self.actor_lr_scheduler, self.actor_model_config = self._build_model_optimizer(
                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/ragen/workers/fsdp_workers.py", line 278, in _build_model_optimizer
    actor_module_fsdp = FSDP(
                        ^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
    _auto_wrap(
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 101, in _auto_wrap
    _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
                                        ^^^^^^^^^^^^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
                                        ^^^^^^^^^^^^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
                                        ^^^^^^^^^^^^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/wrap.py", line 563, in _recursive_wrap
    return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/wrap.py", line 492, in _wrap
    return wrapper_cls(module, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
    _init_param_handle_from_module(
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 636, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 661, in _init_param_handle_from_params
    handle.shard()
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 944, in shard
    sharded_flat_param, numel_padded = FlatParamHandle._get_shard(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 1112, in _get_shard
    shard = chunk.clone()
            ^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 47.94 MiB is free. Process 681195 has 21.69 GiB memory in use. Including non-PyTorch memory, this process has 1.93 GiB memory in use. Of the allocated memory 1.29 GiB is allocated by PyTorch, and 240.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
