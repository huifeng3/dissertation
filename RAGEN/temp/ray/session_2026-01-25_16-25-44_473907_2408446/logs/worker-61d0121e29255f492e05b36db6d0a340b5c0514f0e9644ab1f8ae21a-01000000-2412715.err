:job_id:01000000
:actor_name:WorkerDict
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]
[rank0]:[W125 16:26:19.836037802 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.11s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]
/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
Exception ignored in: <function ShmRingBuffer.__del__ at 0x7efdb078c040>
Traceback (most recent call last):
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_broadcast.py", line 150, in __del__
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/multiprocessing/shared_memory.py", line 244, in unlink
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/multiprocessing/resource_tracker.py", line 203, in unregister
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/multiprocessing/resource_tracker.py", line 207, in _send
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/multiprocessing/resource_tracker.py", line 173, in ensure_running
  File "/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/multiprocessing/util.py", line 451, in spawnv_passfds
ImportError: sys.meta_path is None, Python is likely shutting down
[rank0]:[W125 16:27:59.013106563 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
