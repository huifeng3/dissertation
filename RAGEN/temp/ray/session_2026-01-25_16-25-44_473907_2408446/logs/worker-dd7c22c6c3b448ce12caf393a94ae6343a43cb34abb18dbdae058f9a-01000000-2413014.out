:job_id:01000000
:actor_name:WorkerDict
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
wrap_policy: functools.partial(<function _or_policy at 0x7fdb59931bc0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fdb59931a80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
Actor use_remove_padding=True
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
wrap_policy: functools.partial(<function _or_policy at 0x7fdb59931bc0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fdb59931a80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
Total steps: 260, num_warmup_steps: 20
Actor use_remove_padding=True
WARNING 01-25 16:27:39 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
WARNING 01-25 16:27:40 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd9684c62a0>
WARNING 01-25 16:27:41 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
