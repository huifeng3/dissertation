[2026-02-06 22:25:01,583 I 3884468 3884468] core_worker_process.cc:777: Constructing CoreWorkerProcess. pid: 3884468
[2026-02-06 22:25:01,586 I 3884468 3884468] event.cc:499: Ray Event initialized for CORE_WORKER
[2026-02-06 22:25:01,586 I 3884468 3884468] event.cc:499: Ray Event initialized for EXPORT_TASK
[2026-02-06 22:25:01,586 I 3884468 3884468] event.cc:332: Set ray event level to warning
[2026-02-06 22:25:01,586 I 3884468 3884468] event_aggregator_client.h:50: Initiating the local event aggregator client with port: 64190
[2026-02-06 22:25:01,589 I 3884468 3884468] grpc_server.cc:143: worker server started, listening on port 46455.
[2026-02-06 22:25:01,602 I 3884468 3884468] core_worker_process.cc:262: Initializing worker at address: 172.28.6.5:46455 worker_id=76a6e3eac267b11a07be035d3d4e8c7c8d073cf8851a1c6d1434c8f0 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:01,602 I 3884468 3884468] task_event_buffer.cc:480: Reporting task events to GCS every 1000ms.
[2026-02-06 22:25:01,603 I 3884468 3884468] core_worker.cc:515: Adjusted worker niceness to 15
[2026-02-06 22:25:01,604 I 3884468 3884468] metrics_agent_client.cc:45: Initializing exporter ...
[2026-02-06 22:25:01,604 I 3884468 3884512] core_worker.cc:455: Event stats:


Global stats: 12 total (10 active)
Queueing time: mean = 0.00ms, max = 0.01ms, min = 0.01ms, total = 0.03ms
Execution time:  mean = 0.00ms, total = 0.05ms
Event stats:
	PeriodicalRunner.RunFnPeriodically - 7 total (5 active, 1 running), Execution time: mean = 0.01ms, total = 0.05ms, Queueing time: mean = 0.00ms, max = 0.01ms, min = 0.01ms, total = 0.03ms
	CoreWorker.ExitIfParentRayletDies - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	ray::rpc::WorkerInfoGcsService.grpc_client.AddWorkerInfo - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	Publisher.CheckDeadSubscribers - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms

-----------------
Task execution event stats:

Global stats: 0 total (0 active)
Queueing time: mean = -nanms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
Execution time:  mean = -nanms, total = 0.00ms
Event stats:

-----------------
Task Event stats:

IO Service Stats:

Global stats: 4 total (1 active)
Queueing time: mean = 0.01ms, max = 0.02ms, min = 0.01ms, total = 0.03ms
Execution time:  mean = 0.26ms, total = 1.04ms
Event stats:
	CoreWorker.deadline_timer.flush_task_events - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	PeriodicalRunner.RunFnPeriodically - 1 total (0 active), Execution time: mean = 0.28ms, total = 0.28ms, Queueing time: mean = 0.01ms, max = 0.01ms, min = 0.01ms, total = 0.01ms
	ray::rpc::TaskInfoGcsService.grpc_client.AddTaskEventData - 1 total (0 active), Execution time: mean = 0.73ms, total = 0.73ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	ray::rpc::TaskInfoGcsService.grpc_client.AddTaskEventData.OnReplyReceived - 1 total (0 active), Execution time: mean = 0.02ms, total = 0.02ms, Queueing time: mean = 0.02ms, max = 0.02ms, min = 0.02ms, total = 0.02ms
Other Stats:
	gcs_grpc_in_progress:0
	event_aggregator_grpc_in_progress:0
	current number of task status events in buffer: 0
	current number of profile events in buffer: 0
	current number of dropped task attempts tracked: 0
	total task events sent: 0 MiB
	total number of task attempts sent: 0
	total number of task attempts dropped reported: 0
	total number of sent failure: 0
	num status task events dropped: 0
	num profile task events dropped: 0
	num ray task events reported to aggregator: 0
	num ray task events failed to report to aggregator: 0
	num of task attempts dropped reported to aggregator: 0
	num of failed requests to aggregator: 0

[2026-02-06 22:25:01,606 I 3884468 3884512] accessor.cc:433: Received address and liveness notification for node, IsAlive = 1 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:01,606 I 3884468 3884512] normal_task_submitter.cc:840: Number of alive nodes:1
[2026-02-06 22:25:01,606 I 3884468 3884468] actor_task_submitter.cc:75: Set actor max pending calls to -1 actor_id=e624656458ceda4ae045881301000000
[2026-02-06 22:25:01,606 I 3884468 3884468] core_worker.cc:2904: Creating actor actor_id=e624656458ceda4ae045881301000000
[2026-02-06 22:25:04,650 I 3884468 3884512] metrics_agent_client.cc:59: Exporter initialized.
[2026-02-06 22:25:10,938 I 3884468 3884468] task_receiver.cc:142: Actor creation task finished, task_id: ffffffffffffffffe624656458ceda4ae045881301000000, actor_id: e624656458ceda4ae045881301000000, actor_repr_name: 
[2026-02-06 22:25:11,732 I 3884468 3884468] core_worker.cc:2301: Submitting Placement Group creation to GCS placement_group_id=cc5e51288c5904f2db8bd85767ed01000000
[2026-02-06 22:25:12,869 W 3884468 3884468] actor_manager.cc:110: Failed to look up actor with name 'O69Z2QWorkerDict_0:0'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.
[2026-02-06 22:25:12,920 I 3884468 3884468] actor_task_submitter.cc:75: Set actor max pending calls to -1 actor_id=011a5292cb64bc00e5562b7001000000
[2026-02-06 22:25:20,945 I 3884468 3884468] actor_task_submitter.cc:75: Set actor max pending calls to 0 actor_id=9d8725f86580718e6d32519901000000
[2026-02-06 22:25:20,950 I 3884468 3884512] actor_manager.cc:236: received notification on actor, state: PENDING_CREATION, ip address: , port: 0, num_restarts: 0, death context type=CONTEXT_NOT_SET actor_id=9d8725f86580718e6d32519901000000 worker_id=NIL_ID node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,016 I 3884468 3884512] actor_manager.cc:236: received notification on actor, state: ALIVE, ip address: 172.28.6.5, port: 42225, num_restarts: 0, death context type=CONTEXT_NOT_SET actor_id=9d8725f86580718e6d32519901000000 worker_id=58ccb94865e444a2fb041877dede9a7583fa30367ebd60b1e3edd025 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,020 W 3884468 3884468] actor_manager.cc:110: Failed to look up actor with name 'O69Z2QWorkerDict_0:1'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.
[2026-02-06 22:25:22,021 I 3884468 3884468] actor_task_submitter.cc:75: Set actor max pending calls to -1 actor_id=09bb694fcc7106d2a355935001000000
[2026-02-06 22:25:22,039 I 3884468 3884512] actor_manager.cc:236: received notification on actor, state: PENDING_CREATION, ip address: 172.28.6.5, port: 39379, num_restarts: 0, death context type=CONTEXT_NOT_SET actor_id=011a5292cb64bc00e5562b7001000000 worker_id=30167c4f3c047f9a6b3e4d2c8f07a361bd5697f303b3cd0bd5a4fe50 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,040 I 3884468 3884512] actor_manager.cc:236: received notification on actor, state: PENDING_CREATION, ip address: , port: 0, num_restarts: 0, death context type=CONTEXT_NOT_SET actor_id=09bb694fcc7106d2a355935001000000 worker_id=NIL_ID node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,152 I 3884468 3884512] actor_task_submitter.cc:126: Actor creation failed and we will not be retrying the creation task actor_id=011a5292cb64bc00e5562b7001000000 task_id=ffffffffffffffff011a5292cb64bc00e5562b7001000000
[2026-02-06 22:25:22,152 I 3884468 3884512] actor_manager.cc:236: received notification on actor, state: ALIVE, ip address: 172.28.6.5, port: 39379, num_restarts: 0, death context type=CONTEXT_NOT_SET actor_id=011a5292cb64bc00e5562b7001000000 worker_id=30167c4f3c047f9a6b3e4d2c8f07a361bd5697f303b3cd0bd5a4fe50 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,174 I 3884468 3884512] actor_manager.cc:236: received notification on actor, state: DEAD, ip address: 172.28.6.5, port: 42225, num_restarts: 0, death context type=ActorDiedErrorContext actor_id=9d8725f86580718e6d32519901000000 worker_id=58ccb94865e444a2fb041877dede9a7583fa30367ebd60b1e3edd025 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,174 I 3884468 3884512] actor_task_submitter.cc:424: Failing pending tasks for actor because the actor is already dead. actor_id=9d8725f86580718e6d32519901000000
[2026-02-06 22:25:22,175 I 3884468 3884512] actor_manager.cc:236: received notification on actor, state: DEAD, ip address: 172.28.6.5, port: 39379, num_restarts: 0, death context type=CreationTaskFailureContext actor_id=011a5292cb64bc00e5562b7001000000 worker_id=30167c4f3c047f9a6b3e4d2c8f07a361bd5697f303b3cd0bd5a4fe50 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,175 I 3884468 3884512] actor_task_submitter.cc:424: Failing pending tasks for actor because the actor is already dead. actor_id=011a5292cb64bc00e5562b7001000000
[2026-02-06 22:25:22,281 W 3884468 3884512] task_manager.cc:1351: Task attempt 7717a4e00c9e5779011a5292cb64bc00e5562b7001000000 failed with error ACTOR_DIED Fail immediately? 0, status IOError: The actor was restarted, error info actor_died_error {
  creation_task_failure_context {
    serialized_exception: "\200\005\225\315\t\000\000\000\000\000\000\214\016ray.exceptions\224\214\016ActorDiedError\224\223\224h\000\214\014RayTaskError\224\223\224(\214\010__init__\224X\214\004\000\000Traceback (most recent call last):\n  File \"python/ray/_raylet.pyx\", line 1722, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 1665, in ray._raylet.execute_task.function_executor\n  File \"/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/ray/_private/function_manager.py\", line 693, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n    return method(self, *_args, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/TROJail/verl/verl/single_controller/ray/base.py\", line 528, in __init__\n    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get(\"args\", ()), **init_args_dict[key].get(\"kwargs\", {}))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/TROJail/ragen/workers/fsdp_workers.py\", line 87, in __init__\n    gpu_ids = ray.get_gpu_ids()\n              ^^^\nNameError: name \'ray\' is not defined\n\224\214\010builtins\224\214\tNameError\224\223\224\214\031name \'ray\' is not defined\224\205\224R\224\214-ray::O69Z2QWorkerDict_0:0:WorkerDict.__init__\224NNt\224R\224}\224(\214\tproctitle\224h\r\214\003pid\224J\303H;\000\214\002ip\224\214\n172.28.6.5\224\214\rfunction_name\224h\005\214\rtraceback_str\224h\006\214\nactor_repr\224\214E<verl.single_controller.ray.base.WorkerDict object at 0x7eee92932c60>\224\214\t_actor_id\224\214 011a5292cb64bc00e5562b7001000000\224\214\005cause\224h\014ub\205\224R\224}\224(\214\010actor_id\224h\032\214\terror_msg\224XZ\003\000\000The actor died because of an error raised in its creation task, \033[36mray::O69Z2QWorkerDict_0:0:WorkerDict.__init__()\033[39m (pid=3885251, ip=172.28.6.5, actor_id=011a5292cb64bc00e5562b7001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eee92932c60>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/TROJail/verl/verl/single_controller/ray/base.py\", line 528, in __init__\n    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get(\"args\", ()), **init_args_dict[key].get(\"kwargs\", {}))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/TROJail/ragen/workers/fsdp_workers.py\", line 87, in __init__\n    gpu_ids = ray.get_gpu_ids()\n              ^^^\nNameError: name \'ray\' is not defined\224\214\022_actor_init_failed\224\210\214\n_preempted\224\211ub."
    formatted_exception_string: "Traceback (most recent call last):\n\n  File \"python/ray/_raylet.pyx\", line 1715, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1826, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1722, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1665, in ray._raylet.execute_task.function_executor\n\n  File \"/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/ray/_private/function_manager.py\", line 693, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n    return method(self, *_args, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/data1/TROJail/verl/verl/single_controller/ray/base.py\", line 528, in __init__\n    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get(\"args\", ()), **init_args_dict[key].get(\"kwargs\", {}))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/data1/TROJail/ragen/workers/fsdp_workers.py\", line 87, in __init__\n    gpu_ids = ray.get_gpu_ids()\n              ^^^\n\nNameError: name \'ray\' is not defined\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"python/ray/_raylet.pyx\", line 2164, in ray._raylet.task_execution_handler\n\n  File \"python/ray/_raylet.pyx\", line 2023, in ray._raylet.execute_task_with_cancellation_handler\n\n  File \"python/ray/_raylet.pyx\", line 1667, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1668, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1917, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 934, in ray._raylet.store_task_errors\n\nray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \033[36mray::O69Z2QWorkerDict_0:0:WorkerDict.__init__()\033[39m (pid=3885251, ip=172.28.6.5, actor_id=011a5292cb64bc00e5562b7001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eee92932c60>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/TROJail/verl/verl/single_controller/ray/base.py\", line 528, in __init__\n    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get(\"args\", ()), **init_args_dict[key].get(\"kwargs\", {}))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/TROJail/ragen/workers/fsdp_workers.py\", line 87, in __init__\n    gpu_ids = ray.get_gpu_ids()\n              ^^^\nNameError: name \'ray\' is not defined\n"
  }
}
error_message: "Traceback (most recent call last):\n\n  File \"python/ray/_raylet.pyx\", line 1715, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1826, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1722, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1665, in ray._raylet.execute_task.function_executor\n\n  File \"/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/ray/_private/function_manager.py\", line 693, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/data1/hhc/miniconda3/envs/TROJail-test/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 461, in _resume_span\n    return method(self, *_args, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/data1/TROJail/verl/verl/single_controller/ray/base.py\", line 528, in __init__\n    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get(\"args\", ()), **init_args_dict[key].get(\"kwargs\", {}))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/data1/TROJail/ragen/workers/fsdp_workers.py\", line 87, in __init__\n    gpu_ids = ray.get_gpu_ids()\n              ^^^\n\nNameError: name \'ray\' is not defined\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"python/ray/_raylet.pyx\", line 2164, in ray._raylet.task_execution_handler\n\n  File \"python/ray/_raylet.pyx\", line 2023, in ray._raylet.execute_task_with_cancellation_handler\n\n  File \"python/ray/_raylet.pyx\", line 1667, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1668, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 1917, in ray._raylet.execute_task\n\n  File \"python/ray/_raylet.pyx\", line 934, in ray._raylet.store_task_errors\n\nray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \033[36mray::O69Z2QWorkerDict_0:0:WorkerDict.__init__()\033[39m (pid=3885251, ip=172.28.6.5, actor_id=011a5292cb64bc00e5562b7001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eee92932c60>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/TROJail/verl/verl/single_controller/ray/base.py\", line 528, in __init__\n    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get(\"args\", ()), **init_args_dict[key].get(\"kwargs\", {}))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data1/TROJail/ragen/workers/fsdp_workers.py\", line 87, in __init__\n    gpu_ids = ray.get_gpu_ids()\n              ^^^\nNameError: name \'ray\' is not defined\n"
error_type: ACTOR_DIED

[2026-02-06 22:25:22,281 I 3884468 3884512] task_manager.cc:1232: task 7717a4e00c9e5779011a5292cb64bc00e5562b7001000000 retries left: 0, oom retries left: 0, task failed due to oom: 0
[2026-02-06 22:25:22,281 I 3884468 3884512] task_manager.cc:1246: No retries left for task 7717a4e00c9e5779011a5292cb64bc00e5562b7001000000, not going to resubmit.
[2026-02-06 22:25:22,282 I 3884468 3884512] task_manager.cc:1322: Task failed: IOError: The actor was restarted: Type=ACTOR_TASK, Language=PYTHON, Resources: {}, function_descriptor={type=PythonFunctionDescriptor, module_name=verl.single_controller.ray.base, class_name=create_colocated_worker_cls.<locals>.WorkerDict, function_name=ref_init_model, function_hash=}, task_id=7717a4e00c9e5779011a5292cb64bc00e5562b7001000000, task_name=WorkerDict.ref_init_model, job_id=01000000, num_args=0, num_returns=1, max_retries=0, depth=2, attempt_number=0, actor_task_spec={actor_id=011a5292cb64bc00e5562b7001000000, actor_caller_id=ffffffffffffffffe624656458ceda4ae045881301000000, seq_no=0, retry_exceptions=0}, runtime_env_hash=-1739433844, eager_install=1, setup_timeout_seconds=600
[2026-02-06 22:25:22,850 I 3884468 3884512] core_worker.cc:4090: Force kill actor request has received. exiting immediately... The actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 172.28.6.5 Owner worker exit type: INTENDED_USER_EXIT Worker exit detail: Owner's worker process has crashed.
[2026-02-06 22:25:22,871 I 3884468 3884512] core_worker_shutdown_executor.cc:217: Try killing all child processes of this worker as it exits. Child process pids: 
[2026-02-06 22:25:22,871 I 3884468 3884512] core_worker_shutdown_executor.cc:262: Sending disconnect message to the local raylet.
[2026-02-06 22:25:22,871 I 3884468 3884512] raylet_ipc_client.cc:135: RayletIpcClient::Disconnect, exit_type=INTENDED_SYSTEM_EXIT, exit_detail=Worker exits because the actor is killed. The actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 172.28.6.5 Owner worker exit type: INTENDED_USER_EXIT Worker exit detail: Owner's worker process has crashed., has creation_task_exception_pb_bytes=0
