[2026-02-06 22:25:13,751 I 3885251 3885251] core_worker_process.cc:777: Constructing CoreWorkerProcess. pid: 3885251
[2026-02-06 22:25:13,755 I 3885251 3885251] event.cc:499: Ray Event initialized for CORE_WORKER
[2026-02-06 22:25:13,755 I 3885251 3885251] event.cc:499: Ray Event initialized for EXPORT_TASK
[2026-02-06 22:25:13,755 I 3885251 3885251] event.cc:332: Set ray event level to warning
[2026-02-06 22:25:13,755 I 3885251 3885251] event_aggregator_client.h:50: Initiating the local event aggregator client with port: 64190
[2026-02-06 22:25:13,756 I 3885251 3885251] grpc_server.cc:143: worker server started, listening on port 39379.
[2026-02-06 22:25:13,765 I 3885251 3885251] core_worker_process.cc:262: Initializing worker at address: 172.28.6.5:39379 worker_id=30167c4f3c047f9a6b3e4d2c8f07a361bd5697f303b3cd0bd5a4fe50 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:13,766 I 3885251 3885251] task_event_buffer.cc:480: Reporting task events to GCS every 1000ms.
[2026-02-06 22:25:13,767 I 3885251 3885251] core_worker.cc:515: Adjusted worker niceness to 15
[2026-02-06 22:25:13,767 I 3885251 3885251] metrics_agent_client.cc:45: Initializing exporter ...
[2026-02-06 22:25:13,767 I 3885251 3885285] core_worker.cc:455: Event stats:


Global stats: 12 total (10 active)
Queueing time: mean = 0.00ms, max = 0.01ms, min = 0.01ms, total = 0.03ms
Execution time:  mean = 0.00ms, total = 0.05ms
Event stats:
	PeriodicalRunner.RunFnPeriodically - 7 total (5 active, 1 running), Execution time: mean = 0.01ms, total = 0.05ms, Queueing time: mean = 0.00ms, max = 0.01ms, min = 0.01ms, total = 0.03ms
	Publisher.CheckDeadSubscribers - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	ray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	ray::rpc::WorkerInfoGcsService.grpc_client.AddWorkerInfo - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	CoreWorker.ExitIfParentRayletDies - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms

-----------------
Task execution event stats:

Global stats: 0 total (0 active)
Queueing time: mean = -nanms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
Execution time:  mean = -nanms, total = 0.00ms
Event stats:

-----------------
Task Event stats:

IO Service Stats:

Global stats: 4 total (1 active)
Queueing time: mean = 0.01ms, max = 0.05ms, min = 0.01ms, total = 0.06ms
Execution time:  mean = 0.30ms, total = 1.20ms
Event stats:
	ray::rpc::TaskInfoGcsService.grpc_client.AddTaskEventData - 1 total (0 active), Execution time: mean = 0.82ms, total = 0.82ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
	ray::rpc::TaskInfoGcsService.grpc_client.AddTaskEventData.OnReplyReceived - 1 total (0 active), Execution time: mean = 0.04ms, total = 0.04ms, Queueing time: mean = 0.05ms, max = 0.05ms, min = 0.05ms, total = 0.05ms
	PeriodicalRunner.RunFnPeriodically - 1 total (0 active), Execution time: mean = 0.34ms, total = 0.34ms, Queueing time: mean = 0.01ms, max = 0.01ms, min = 0.01ms, total = 0.01ms
	CoreWorker.deadline_timer.flush_task_events - 1 total (1 active), Execution time: mean = 0.00ms, total = 0.00ms, Queueing time: mean = 0.00ms, max = -0.00ms, min = 9223372036854.78ms, total = 0.00ms
Other Stats:
	gcs_grpc_in_progress:0
	event_aggregator_grpc_in_progress:0
	current number of task status events in buffer: 0
	current number of profile events in buffer: 0
	current number of dropped task attempts tracked: 0
	total task events sent: 0 MiB
	total number of task attempts sent: 0
	total number of task attempts dropped reported: 0
	total number of sent failure: 0
	num status task events dropped: 0
	num profile task events dropped: 0
	num ray task events reported to aggregator: 0
	num ray task events failed to report to aggregator: 0
	num of task attempts dropped reported to aggregator: 0
	num of failed requests to aggregator: 0

[2026-02-06 22:25:13,769 I 3885251 3885285] metrics_agent_client.cc:59: Exporter initialized.
[2026-02-06 22:25:13,769 I 3885251 3885285] accessor.cc:433: Received address and liveness notification for node, IsAlive = 1 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:13,770 I 3885251 3885285] normal_task_submitter.cc:840: Number of alive nodes:1
[2026-02-06 22:25:13,770 I 3885251 3885251] actor_task_submitter.cc:75: Set actor max pending calls to -1 actor_id=011a5292cb64bc00e5562b7001000000
[2026-02-06 22:25:13,770 I 3885251 3885251] core_worker.cc:2904: Creating actor actor_id=011a5292cb64bc00e5562b7001000000
[2026-02-06 22:25:20,532 W 3885251 3885251] actor_manager.cc:110: Failed to look up actor with name 'O69Z2Q_register_center'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.
[2026-02-06 22:25:20,588 I 3885251 3885251] actor_task_submitter.cc:75: Set actor max pending calls to -1 actor_id=9d8725f86580718e6d32519901000000
[2026-02-06 22:25:20,595 I 3885251 3885285] actor_manager.cc:236: received notification on actor, state: PENDING_CREATION, ip address: , port: 0, num_restarts: 0, death context type=CONTEXT_NOT_SET actor_id=9d8725f86580718e6d32519901000000 worker_id=NIL_ID node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,016 I 3885251 3885285] actor_manager.cc:236: received notification on actor, state: ALIVE, ip address: 172.28.6.5, port: 42225, num_restarts: 0, death context type=CONTEXT_NOT_SET actor_id=9d8725f86580718e6d32519901000000 worker_id=58ccb94865e444a2fb041877dede9a7583fa30367ebd60b1e3edd025 node_id=45b45d6a05722e1aaaa3ceb67d240de1b43438b1dc0815f6981b43ce
[2026-02-06 22:25:22,150 I 3885251 3885251] core_worker_shutdown_executor.cc:94: Executing worker exit: USER_ERROR - Worker exits because there was an exception in the initialization method (e.g., __init__). Fix the exceptions from the initialization to resolve the issue. Exception raised from an actor init method. Traceback: The actor died because of an error raised in its creation task, [36mray::O69Z2QWorkerDict_0:0:WorkerDict.__init__()[39m (pid=3885251, ip=172.28.6.5, actor_id=011a5292cb64bc00e5562b7001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eee92932c60>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/verl/verl/single_controller/ray/base.py", line 528, in __init__
    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get("args", ()), **init_args_dict[key].get("kwargs", {}))
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/ragen/workers/fsdp_workers.py", line 87, in __init__
    gpu_ids = ray.get_gpu_ids()
              ^^^
NameError: name 'ray' is not defined (timeout: -1ms)
[2026-02-06 22:25:22,150 W 3885251 3885251] task_receiver.cc:134: Actor creation task finished with errors, task_id: ffffffffffffffff011a5292cb64bc00e5562b7001000000, actor_id: 011a5292cb64bc00e5562b7001000000, status: CreationTaskError: Exception raised from an actor init method. Traceback: The actor died because of an error raised in its creation task, [36mray::O69Z2QWorkerDict_0:0:WorkerDict.__init__()[39m (pid=3885251, ip=172.28.6.5, actor_id=011a5292cb64bc00e5562b7001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eee92932c60>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/verl/verl/single_controller/ray/base.py", line 528, in __init__
    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get("args", ()), **init_args_dict[key].get("kwargs", {}))
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/ragen/workers/fsdp_workers.py", line 87, in __init__
    gpu_ids = ray.get_gpu_ids()
              ^^^
NameError: name 'ray' is not defined
[2026-02-06 22:25:22,151 I 3885251 3885251] core_worker_shutdown_executor.cc:128: Wait for currently executing tasks in the underlying thread pools to finish.
[2026-02-06 22:25:22,151 I 3885251 3885251] core_worker_shutdown_executor.cc:170: Not draining reference counter since this is an actor worker.
[2026-02-06 22:25:22,172 I 3885251 3885251] core_worker_shutdown_executor.cc:217: Try killing all child processes of this worker as it exits. Child process pids: 
[2026-02-06 22:25:22,172 I 3885251 3885251] core_worker_shutdown_executor.cc:262: Sending disconnect message to the local raylet.
[2026-02-06 22:25:22,172 I 3885251 3885251] raylet_ipc_client.cc:135: RayletIpcClient::Disconnect, exit_type=USER_ERROR, exit_detail=Worker exits because there was an exception in the initialization method (e.g., __init__). Fix the exceptions from the initialization to resolve the issue. Exception raised from an actor init method. Traceback: The actor died because of an error raised in its creation task, [36mray::O69Z2QWorkerDict_0:0:WorkerDict.__init__()[39m (pid=3885251, ip=172.28.6.5, actor_id=011a5292cb64bc00e5562b7001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eee92932c60>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/verl/verl/single_controller/ray/base.py", line 528, in __init__
    self.worker_dict[key] = user_defined_cls(*init_args_dict[key].get("args", ()), **init_args_dict[key].get("kwargs", {}))
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data1/TROJail/ragen/workers/fsdp_workers.py", line 87, in __init__
    gpu_ids = ray.get_gpu_ids()
              ^^^
NameError: name 'ray' is not defined, has creation_task_exception_pb_bytes=1
[2026-02-06 22:25:22,173 I 3885251 3885251] core_worker_shutdown_executor.cc:279: Disconnected from the local raylet.
[2026-02-06 22:25:22,173 I 3885251 3885251] task_event_buffer.cc:491: Shutting down TaskEventBuffer.
[2026-02-06 22:25:22,173 I 3885251 3885305] task_event_buffer.cc:459: Task event buffer io service stopped.
[2026-02-06 22:25:22,173 I 3885251 3885251] core_worker_shutdown_executor.cc:54: Waiting for joining a core worker io thread. If it hangs here, there might be deadlock or a high load in the core worker io service.
[2026-02-06 22:25:22,173 I 3885251 3885285] core_worker_process.cc:195: Core worker main io service stopped.
[2026-02-06 22:25:22,177 I 3885251 3885251] core_worker_shutdown_executor.cc:72: Disconnecting a GCS client.
[2026-02-06 22:25:22,177 I 3885251 3885251] core_worker_shutdown_executor.cc:79: Core worker ready to be deallocated.
[2026-02-06 22:25:22,177 I 3885251 3885251] core_worker_process.cc:955: Task execution loop terminated. Removing the global worker.
[2026-02-06 22:25:22,177 I 3885251 3885251] core_worker.cc:539: Core worker is destructed
[2026-02-06 22:25:22,177 I 3885251 3885251] task_event_buffer.cc:491: Shutting down TaskEventBuffer.
[2026-02-06 22:25:22,178 I 3885251 3885251] core_worker_process.cc:851: Destructing CoreWorkerProcessImpl. pid: 3885251
[2026-02-06 22:25:22,180 I 3885251 3885251] stats.h:149: Stats module has shutdown.
