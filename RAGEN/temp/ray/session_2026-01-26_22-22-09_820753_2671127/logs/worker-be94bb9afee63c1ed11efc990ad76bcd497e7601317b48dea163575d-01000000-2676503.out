:job_id:01000000
:actor_name:WorkerDict
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
wrap_policy: functools.partial(<function _or_policy at 0x7f5ae1dd1bc0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f5ae1dd1a80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
Actor use_remove_padding=True
Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
wrap_policy: functools.partial(<function _or_policy at 0x7f5ae1dd1bc0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f5ae1dd1a80>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
Total steps: 260, num_warmup_steps: 20
Actor use_remove_padding=True
WARNING 01-26 22:23:15 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
WARNING 01-26 22:23:16 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f58c9f46570>
WARNING 01-26 22:23:17 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 128, 'detokenize': False, 'temperature': 1, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
